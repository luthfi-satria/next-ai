# Dummy Data Import Guide for Elasticsearch & MongoDB

This guide explains the steps to generate dummy data and import it into both Elasticsearch and MongoDB.

## 1. Generate Dummy Data

Before importing data, you need to run a Python script to generate dummy data in a format suitable for both databases. Make sure you have saved the dummy data generator Python script with the name `dummy-store-generator.py`.

**Command:**

python dummy-store-generator.py

* **Script Output:**
    This script will print progress to the console and generate two JSON files in the same directory where you run the script:
    * `stores_data_elasticsearch_YYYYMMDD_HHMMSS.json` (for Elasticsearch)
    * `stores_data_mongo_YYYYMMDD_HHMMSS.json` (for MongoDB)
    * `YYYYMMDD_HHMMSS` will be replaced with the date and time when the file was created (e.g., `20250709_142429`).

## 2. Import Data to Elasticsearch

You will use the Elasticsearch Bulk API to import the generated data. Ensure your Elasticsearch server is running.

**Command (Replace Placeholders):**

curl -H "Content-Type: application/x-ndjson" -XPOST "localhost:9200/[index_name]/_bulk" --data-binary "@your_elasticsearch_filename.json"

**Parameter Explanation:**

* `localhost:9200`: Your Elasticsearch address. Change if your Elasticsearch is running on a different host or port.
* `[index_name]`: The name of the Elasticsearch index where you want to store the data (e.g., `stores_index`).
* `your_elasticsearch_filename.json`: The name of the JSON file generated by your Python script (e.g., `stores_data_elasticsearch_20250709_142429.json`). You must replace the timestamp accordingly.
* `--data-binary "@"`: Instructs `curl` to read the request body from a file.
* `-H "Content-Type: application/x-ndjson"`: Specifies the content type for the Elasticsearch Bulk API.

**Example Command (with example filename):**

curl -H "Content-Type: application/x-ndjson" -XPOST "localhost:9200/stores_index/_bulk" --data-binary "@stores_data_elasticsearch_20250709_142429.json"

* **Important Notes:**
    * Ensure your Elasticsearch index (e.g., `stores_index`) is created with the appropriate mapping. Specifically, the `location` field should be of type `geo_point` for proper geographic data indexing.
    * If your Elasticsearch uses authentication, you will need to add `-u <username>:<password>` or `--header "Authorization: ApiKey <base64_api_key>"` options to your `curl` command.

## 3. Import Data to MongoDB (in Docker)

For MongoDB running inside a Docker container, you need to copy the data file into the container first, and then execute the `mongoimport` command within that container.

### 3.1. Copy Dummy Data File to MongoDB Docker Container

**Command (Replace Placeholders):**

docker cp your_mongo_filename.json your_mongo_container_name:/tmp/dummy_store_mongo.json

**Parameter Explanation:**

* `your_mongo_filename.json`: The name of the JSON file generated by your Python script (e.g., `stores_data_mongo_20250709_142429.json`).
* `your_mongo_container_name`: The name or ID of your MongoDB Docker container. You can find this by running `docker ps`.
* `/tmp/dummy_store_mongo.json`: The destination path inside the container where the file will be copied.

**Example Command (with example filename and container name):**

docker cp stores_data_mongo_20250709_142429.json my-mongo-container:/tmp/dummy_store_mongo.json

### 3.2. Run `mongoimport` inside the Docker Container

**Command (Replace Placeholders):**

docker exec your_mongo_container_name mongoimport \
  --db DATABASE_NAME \
  --collection COLLECTION_NAME \
  --file /tmp/dummy_store_mongo.json \
  --ndjson \
  --drop \
  --username DB_USERNAME \
  --password DB_PASSWORD \
  --authenticationDatabase admin

**Parameter Explanation:**

* `your_mongo_container_name`: The name or ID of your MongoDB Docker container.
* `DATABASE_NAME`: The name of the MongoDB database where you want to import data (e.g., `next_ai`).
* `COLLECTION_NAME`: The name of the MongoDB collection where the data will be stored (e.g., `stores`).
* `--file /tmp/dummy_store_mongo.json`: The path to the JSON file inside the container.
* `--ndjson`: This option tells `mongoimport` that each line in the file is a separate JSON document. This is important for files generated by the Python script.
    * **If your `mongoimport` errors with `--ndjson` (e.g., `unknown option "ndjson"`):** This means your `mongoimport` version is too old. You can try removing `--ndjson` or update your MongoDB Database Tools.
* `--drop` (optional): Deletes all existing documents in `COLLECTION_NAME` before importing new data. **Use this option with extreme caution in a production environment.**
* `--username DB_USERNAME`: The MongoDB username with permissions to write to the specified database and collection.
* `--password DB_PASSWORD`: The password for `DB_USERNAME`.
* `--authenticationDatabase admin`: The database where the `DB_USERNAME` user is authenticated. This is often `admin`, but it could be another database if the user was defined there.

**Example Command (with example container name, database, collection, and credentials):**

docker exec my-mongo-container mongoimport \
  --db next_ai \
  --collection stores \
  --file /tmp/dummy_store_mongo.json \
  --ndjson \
  --drop \
  --username user \
  --password root \
  --authenticationDatabase admin